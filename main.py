from openai import OpenAI
from typing import List, Optional, Tuple
import json
# from config import opeani_api_keys
import pandas as pd
from auto_plot_mat import auto_plot
from pandasql import sqldf
#åŠ è½½ .env åˆ°ç¯å¢ƒå˜é‡
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())

from tabulate import tabulate
import gradio as gr
import argparse
def get_args():
        parser = argparse.ArgumentParser()
        parser.add_argument("--openai_key", type=str,
                        default="", help="openai-keys")
        args = parser.parse_args()
        return args
args = get_args()
openai_key = args.openai_key
temperature=0.2
History = List[Tuple[str, str]]
def process_file(file):
    if not isinstance(file, list):
        file = [file]
    df = pd.DataFrame()
    for i in range(len(file)):
        data_temp = {'new_df' + str(i): {'type': file[i].name, 'description': '-'*10}}
        df_temp = pd.DataFrame.from_dict(data_temp, orient='index').reset_index()
        df = pd.concat([df, df_temp], axis=0, ignore_index=True)
        df_real = pd.read_csv(file[i].name)
        
        schema_info = generate_schema(df_real)
        df_schema = pd.DataFrame.from_dict(schema_info, orient='index').reset_index()
        df = pd.concat([df, df_schema], axis=0, ignore_index=True)
    return df


def generate_schema(df_real):
    df_string = df_real.head().to_string()
    prompt = """
    è¯·ä½ æ ¹æ®ä¸€ä¸ªdataframeçš„å‰5è¡Œå±•ç¤ºçš„ä¿¡æ¯ï¼Œæ¥å†™ä¸€ä¸ªå®ƒçš„schema_infoï¼Œschema_infoæ˜¯ä¸€ä¸ªjsonä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

    ########################
    dataframeçš„å‰5è¡Œ:
            Customer ID  Age  Gender Marital Status    Education Level
    0        84966   23  female        married   associate degree
    1        95568   26    male        widowed          doctorate
    2        10544   29  female         single   associate degree
    3        77033   20    male       divorced  bachelor's degree
    4        88160   25  female      separated  bachelor's degree
    schema_info:
    {
        'Customer ID': {'type': 'int', 'description': 'Unique identifier for each customer'},
        'Age': {'type': 'int', 'description': 'The age of the customer'},
        'Gender': {'type': 'str', 'description': 'The gender of the customer'},
        'Marital Status': {'type': 'str', 'description': 'The marital status of the customer'},
        'Education Level': {'type': 'str', 'description': 'The highest level of education achieved by the customer'}
    }
    ########################
    dataframeçš„å‰5è¡Œ:
    """ + df_string + '\n' + 'schema_info:'
    schema_info = run_llm(prompt)
    try:
        result = json.loads(schema_info.replace("'", "\""))
    except Exception as e:
        result = {}
        column_types = df_real.dtypes
        for col in df_real.columns:
            result[col] = {
                "type": str(column_types[col]),
                "description": str(df_real[col][0])  # ä½¿ç”¨ç¬¬ä¸€è¡Œçš„æ•°æ®ä½œä¸ºæè¿°
            }
        print(f"a error occured when transfer the schema_info string to dict: {e}, it will use the row's data to be the description")
    return result



def clear_session() -> History:
    return '', []
def generate_sql(query, table_name, schema, history = [], debug = False, error =''):
    if not debug:
        prompt_prefix = '''
        æ‚¨æ˜¯ä¸€ä¸ªsqlalchemyä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹çš„è¡¨åå’Œç›¸åº”çš„columnç”Ÿæˆä¸ç»™å®šé—®é¢˜ç›¸å¯¹åº”çš„SQLè¯­å¥, SQLè¯­å¥å¯ä»¥ä»ä¸€ä¸ªè¡¨æˆ–è€…å¤šä¸ªè¡¨é€‰æ‹©æ•°æ®ï¼Œå¯ä»¥ç”¨joinæˆ–è€…å­æŸ¥è¯¢æ¯”è¾ƒç­‰ç­‰æ–¹æ³•, SQLè¯­å¥ä¸è¦å¸¦æœ‰æ¢è¡Œç¬¦, è¡¨åä¸èƒ½å¸¦å•å¼•å·,å¦‚df å°±æ˜¯ df ,
        å¼ºè°ƒ:æ¯ä¸ªschemaçš„å¼•ç”¨éƒ½è¦åŠ ä¸Šå•å¼•å·, å¦‚'schema'ã€‚
        
        ä¸‹é¢æ˜¯é—®é¢˜å’Œå¯¹åº”çš„SQLè¯­å¥ç¤ºä¾‹:
        ########################
        é—®é¢˜: æœ¬ç§‘å­¦å†çš„äººçš„å¹³å‡å·¥èµ„æ˜¯å¤šå°‘ï¼Ÿ
        SQLè¯­å¥:  SELECT avg(df.'Income Level') from df where df.'Education Level' like '%bachelor%';
        
        é—®é¢˜: ä¸åŒåœ°åŒºçš„å¹³å‡å·¥èµ„æ˜¯å¤šå°‘ï¼Ÿ
        SQLè¯­å¥: SELECT df.'Geographic Information',avg(df.'Income Level') from df group by df.'Geographic Information';

        é—®é¢˜: è¯·ä½ å•èº«çš„äººæ•°æœ‰å¤šå°‘ï¼Ÿ
        SQLè¯­å¥: SELECT count('Customer ID') from df where df.'Marital Status' like '%single%';;
        ########################
        
        é—®é¢˜: {}
        è¡¨åï¼š{}
        schema: {}
        
        SQLè¯­å¥:     
        '''
        # table_name = list(locals().keys())[list(locals().values()).index(df.items())]

        prompt_content = prompt_prefix.format(query, table_name, schema)
        answer = run_llm(prompt_content)
        history.append({'role':'user',"content":prompt_content})
        history.append({'role':'assistant',"content":answer})
        
        print(f'sql_answer******************{answer}')
        return answer, history
    else:
        prompt_debug = """ä¹‹å‰çš„sqlç”Ÿæˆè¯­å¥:{}, æŠ¥é”™äº†{},è¯·é‡æ–°ç”Ÿæˆsqlè¯­å¥,å›ç­”è¦æ˜¯çº¯çš„sqlè¯­å¥:
        """
        sql_before = error.split('...')[1]
        e = error.split('...')[0]
        prompt_debug_content = prompt_debug.format(sql_before, e)
        answer_debug = run_llm(prompt_debug_content, history=history)
        history.append({'role':'user',"content":prompt_debug_content})
        history.append({'role':'assistant',"content":answer_debug})
        return answer_debug, history

def run_llm(query, openai_key=openai_key, api_base='', engine='',history = []):
    
    client = OpenAI(
            api_key=openai_key,  # this is also the default, it can be omitted
        )
    
    messages = [{"role":"system","content":"You are an useful AI assistant that helps people solve the problem step by step."}]
    messages.extend(history)
    message_prompt = {"role":"user","content":query}
    messages.append(message_prompt)
    engine="gpt-3.5-turbo"
    response = client.chat.completions.create(
                    model=engine,
                    messages = messages,
                    temperature=0.5,
                    # max_tokens=max_tokens,
                    frequency_penalty=0,
                    presence_penalty=0)

    result = response.choices[0].message.content
    return result

def model_chat(query: Optional[str], history: Optional[History], df_process):
    split_indices = df_process.index[df_process['description'] == '-'*10].tolist()
    paths = []
    
    # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œåˆ™åªæœ‰ä¸€ä¸ªDataFrame
    if not split_indices:
        dfs = [df_process]
    else:
        dfs = []
        start_idx = 0
        for end_idx in split_indices:
            if end_idx != 0:
                dfs.append(df_process.iloc[start_idx:end_idx])
            start_idx = end_idx + 1
            paths.append(df_process.loc[end_idx, 'type'])
        dfs.append(df_process.iloc[start_idx:])
    # æ‰“å°æ¯ä¸ªDataFrame
    table = []
    scema = ''
    for i, d in enumerate(dfs):
        df_name_str = "df_use_" + str(i+1)
        table.append(df_name_str)
        locals()[df_name_str] = pd.read_csv(paths[i])
        scema_temp = d.set_index('index').to_dict(orient='index')
        scema += df_name_str + 'çš„schema:' + '\n\n' + str(scema_temp) + '\n\n'
    table_name = ','.join(table)
    sql_statement, sql_history = generate_sql(query, table_name, scema)
    print(f"sql_statement**********************{sql_statement}")
    i = 0
    while i < 3:
        print(f"i_______________________{i}")
        try:
            pandaSQL_solution = sqldf(sql_statement, locals())
            break
        except Exception as e:
            print(f"there're bugs in sql, debuging by the gpt at the {i+1} time......")
            e = str(e) + '...' + str(sql_statement)
            sql_statement, sql_history = generate_sql(query, table_name, scema, history=sql_history, debug=True, error = e)
            print(f"{i+1} time fixed sql:{sql_statement}")
        i+=1

    
    image_arrays = auto_plot(df=pandaSQL_solution)

    mylist = list()
    mylist.append(query)
    prompt_list = [query]
    string_data = pandaSQL_solution.to_string()
    prompt_list.append(sql_statement)
    prompt_list.append(string_data)
    prompt_report = '''ç°åœ¨æœ‰ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå€¼æ˜¯é—®é¢˜ï¼Œç¬¬äºŒä¸ªå€¼æ˜¯ç”¨å¤§æ¨¡å‹å®ç°çš„text to sql, ç¬¬ä¸‰ä¸ªå€¼æ˜¯æ ¹æ®sqlæŸ¥å‡ºæ¥çš„æ•°æ®ï¼Œè¯·ä½ æ ¹æ®é—®é¢˜å’ŒæŸ¥å‡ºæ¥çš„æ•°æ®ï¼Œå½¢æˆä¸€ä¸ªæ•°æ®åˆ†ææŠ¥å‘Šè¿›è¡Œå›ç­”ï¼Œè¿™ä¸ªåˆ†ææŠ¥å‘Šè¦è¯¦ç»†æ¸…æ™°ï¼Œæœ‰é€»è¾‘å’Œæ¡ç†:{}'''.format(str(prompt_list))
    report_answer = run_llm(query=prompt_report)
    mylist.append(report_answer)
    responses = list()
    if len(history) > 0:
        for history_msg in history:
            responses.append(history_msg)
    responses.append(mylist)
    return "", responses, image_arrays, pandaSQL_solution

with gr.Blocks(css="footer {visibility: hidden}") as demo:
    gr.Markdown("""<center><font size=10>Data Agentã€‚</center>""")
    with gr.Row(equal_height=True):
            file_output = gr.Files()
            df_process = gr.DataFrame()
            file_output.upload(process_file, inputs= [file_output],outputs=[df_process])
    gallery = gr.Gallery(label="æœ€ç»ˆçš„ç»“æœå›¾ç‰‡").style(height='auto',columns=4)
    df = gr.outputs.Dataframe(type='pandas')
    chatbot = gr.Chatbot(label='æŠ¥å‘Šå±•ç¤º')
    textbox = gr.Textbox(lines=2, label='è¯·æè¿°æ‚¨æƒ³æŸ¥è¯¢çš„æ•°æ®')
    
    
    with gr.Row():
        with gr.Column(scale=2):
            clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
        with gr.Column(scale=2):
            sumbit = gr.Button("ğŸš€ å‘é€")
    
    sumbit.click(model_chat, [textbox, chatbot, df_process], [textbox, chatbot, gallery, df])
    clear_history.click(fn=clear_session,
                        inputs=[],
                        outputs=[textbox, chatbot])
    

    

# demo.queue(api_open=False).launch(max_threads=10, height=800, share=False, server_name="0.0.0.0", server_port=8888)
demo.queue(api_open=False).launch(max_threads=10, height=800, share=False)